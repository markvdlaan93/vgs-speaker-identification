<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>TeXtidote analysis</title>
<style type="text/css">
body {
  font-family: sans-serif;
}
.highlight, .highlight-sh, .highlight-spelling {
  padding: 2pt;
  border-radius: 4pt;
  cursor: help;
  opacity: 0.7;
  border: dashed 1px;
}
.highlight {
  background-color: orange;
  color: black;
}
.highlight-sh {
  background-color: yellow;
  color: black;
}
.highlight-spelling {
  background-color: red;
  color: white;
}
div.original-file {
  font-family: monospace;
  font-size: 11pt;
  background-color: #f8f8ff;
  padding: 20pt;
  border-radius: 6pt;
}
.textidote {
  	background-image: url(data:image/svg+xml;base64,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);
}
h1.textidote {
  width: 378px;
  height: 68px;
  display: block;
}
.keyword1 {
  font-weight: bold;
  color: green;
}
.keyword2 {
  font-weight: bold;
  color: darkblue;
}
.comment, .comment * {
  color: darkred;
  font-weight: normal;
}
.no-text {
  display: none;
}
</style>
</head>
<body>
<a href="https://sylvainhalle.github.io/textidote"><h1 class="textidote"><span class="no-text">Results of TeXtidote analysis</span></h1></a>
<p>Here is the result of analyzing your LaTeX file with TeXtidote. Hover the mouse over highlighted portions of the document to read a tooltip that gives you some writing advice.</p>
<p>Found 79 warning(s)</p>
<div class="original-file">
<span class="keyword1">\documentclass</span>[a4paper, oneside]{book}<br/>
<br/>
\bibliographystyle{IEEEtran}<br/>
<span class="comment">%<span class="keyword1">\usepackage</span>{cite}</span><br/>
<br/>
<span class="comment">%% Language and font encodings</span><br/>
<span class="keyword1">\usepackage</span>[english]{babel}<br/>
<span class="keyword1">\usepackage</span>[utf8x]{inputenc}<br/>
<span class="keyword1">\usepackage</span>[T1]{fontenc}<br/>
<br/>
<span class="comment">% Palatino for rm and math | Helvetica for ss | Courier for tt</span><br/>
<span class="keyword1">\usepackage</span>{mathpazo} <span class="comment">% math &amp; rm</span><br/>
\linespread{1.05}        <span class="comment">% Palatino needs more leading (space between lines)</span><br/>
<span class="keyword1">\usepackage</span>[scaled]{helvet} <span class="comment">% ss</span><br/>
<span class="keyword1">\usepackage</span>{courier} <span class="comment">% tt</span><br/>
\normalfont<br/>
<span class="keyword1">\usepackage</span>[T1]{fontenc}<br/>
<br/>
<span class="comment">%% Sets page size and margins</span><br/>
<span class="keyword1">\usepackage</span>[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}<br/>
<br/>
<span class="comment">%% Useful packages</span><br/>
<span class="keyword1">\usepackage</span>{amsmath}<br/>
<span class="keyword1">\usepackage</span>{longtable,array}<br/>
\newcounter{ltrow}<br/>
<span class="keyword1">\usepackage</span>{float}<br/>
<span class="keyword1">\usepackage</span>[colorinlistoftodos]{todonotes}<br/>
<span class="keyword1">\usepackage</span>[colorlinks=false, allcolors=blue]{hyperref}<br/>
<span class="keyword1">\usepackage</span>{graphicx}<br/>
\graphicspath{ {images/} }<br/>
<span class="keyword1">\usepackage</span>{rotating}<br/>
<span class="keyword1">\usepackage</span>{enumitem}<br/>
<span class="keyword1">\usepackage</span>[titletoc]{appendix}<br/>
<br/>
<span class="comment">%% Custom settings</span><br/>
\setlength\parindent{0pt}<br/>
<br/>
<span class="comment">%% Title and author</span><br/>
<span class="keyword1">\title</span>{<br/>
    {<span class="keyword1">\textbf</span>{Analyzing Recurrent Neural Networks based on Visually Grounded Speech signals: encoding of speaker identity}}<span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
    \vspace{2cm}<br/>
	{<br/>
	<span class="keyword2">\begin{figure}</span>[h]<br/>
	\centering<br/>
	<span class="keyword1">\includegraphics</span>[scale=0.15]{images/Tilburg_University_logo.png}\\<br/>
<span class="highlight-sh" title="This figure is missing a label [sh:figref]">	</span><span class="keyword2">\end{figure}</span><br/>
	}<br/>
}<br/>
\author{<br/>
Master's thesis <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
Communication and Information Sciences <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
Specialization track Data Science: Business and Governance <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
Tilburg University - School of Humanities <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\\</span>\vspace{1cm}\\<br/>
Mark van der Laan <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\\</span> <br/>
<span class="keyword1">\textit</span>{\href{mailto:m.l.vdrlaan@tilburguniversity.edu}{m.l.vdrlaan@tilburguniversity.edu}} <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\\</span> ANR: 633762 \\\vspace{1cm}\\ <br/>
Supervisor: dr. G. ChrupaŁa <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\\</span> <br/>
Second reader: dr. D. Hendrickson}<br/>
<br/>
<span class="keyword2">\begin{document}</span><br/>
<br/>
<span class="keyword1">\maketitle</span><br/>
\chapter*{Abstract}<br/>
<br/>
This thesis presents research on how the unique characteristics of voice are encoded in a Recurrent Neural Network (RNN) trained on Visually Grounded Speech signals. Multiple experiments were performed in order to determine if and to what extent speaker identity is encoded. These experiments were executed against raw MFCC vectors, a convolutional layer and the recurrent layers of the neural network. This thesis also describes the process of labeling gender in both the Flickr8K and the Places 205 audio caption dataset. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
The most important takeaway is that the presented model identifies speakers and classify their gender based on manually labelling of both datasets. Moreover, encoding of speaker identification and gender are most prevalent in the first few layers of the neural network. This finding aligns with the research that has been done on encoding of phonemes in this type of model: ‘form-related aspects’ are most prevalent in the first few layers while semantics are better encoded in the deeper layers. Experiments regarding gender classification reveal that there is a difference in accuracy between male and female (gender bias). The gender bias experiment shows that, contrary to the recurrent layers, the raw MFCC vectors and convolutional layer indeed have accuracy difference between gender. <br/>
<br/>
\chapter*{Preface}<br/>
<br/>
This thesis is written to fulfill the master program of Data Science: Business and Governance which is a specialization track within the master program of Communication and Information Sciences. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
I would like to thank my supervisor dr. Gregorz Chrupa\l a for all the work he has done to make the execution of this thesis possible and the valuable feedback that he gave me. With help of dr. Chrupa\l a the Neural Network presented in \autoref{chapter:methods-and-datasets} was trained so that I could use the output to execute this thesis. Besides, I also would like to thank Alessandro Hazenberg for helping me to label the data and my family for their support. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
Yours sincerely,<br/>
<span class="keyword2">\begin{figure}</span>[H]<br/>
<span class="keyword1">\includegraphics</span>[scale=0.1]{images/sign.png}<br/>
<span class="highlight-sh" title="This figure is missing a label [sh:figref]">\</span>end{figure}<br/>
Mark van der Laan <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<span class="keyword1">\textit</span>{Tilburg \today}<br/>
<br/>
\tableofcontents<br/>
<br/>
\listoftables<br/>
<br/>
\listoffigures<br/>
<br/>
<span class="comment">% CHAPTER INTRODUCTION</span><br/>
\chapter{Introduction}<span class="keyword1">\label</span>{chapter:introduction}<br/>
<br/>
The scientific field of computational linguistics is concerned with finding methods to systematically and algorithmically acquire important characteristics of natural language like syntax and semantics. A subfield of computational linguistics is speaker recognition which can be defined as: <span class="keyword1">\textit</span>{‘extract, characterize and recognize the information in the speech signal conveying speaker identity’} \cite{reynolds2002overview}. Within the field of speaker recognition multiple facets are researched like speaker identification (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>identify a speaker from a known set of speakers), speaker verification (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>reject the identity of the speaker based on the provided signal) and speaker diarisation (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>identify speakers in speech signals with multiple speakers) \cite{reynolds2002overview} \cite{tranter2006overview}. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
Within the subfield of speaker recognition, this thesis mainly focuses on speaker identification. More specifically, speaker identification is researched within the context of neural networks trained on Visually Grounded Speech signals. This type of neural network is trained on visual (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>images) and auditory input (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>spoken captions) to learn syntax and semantics from continuous speech. Contrary to older methods, like using text transcriptions, this method requires less supervision and it becomes possible to learn language-independent \cite{HarwathG15}. From a societal perspective this is interesting because with this method less spoken languages can also be analyzed on their syntax and semantics. Moreover, some spoken dialects do not have a specific writing system so methods based on text transcriptions cannot encode these spoken dialects. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
Within the context of neural networks based on Visually Grounded Speech signals it is currently not known how speaker identification is encoded. Therefore, the research question for thesis is: <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
<span class="keyword1">\textit</span>{‘How will speaker identity be encoded in Recurrent Neural Networks based on Visually Grounded Speech signals?’} <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
<br/>
Previous work looked into this type of neural network to see how phonemes are encoded. Researchers \cite{AlishahiBC17} found out that phoneme encoding follows a certain pattern which is that ‘form-related aspects’ are more prevalent in the first few layers of the neural network while all the semantic information is better encoded in the deeper layers. The hypothesis for this research question is that speaker identification also follows this pattern. This implies that the first layers encode speakers better due to the fact that speaker identification is a form-related aspect. This finding aligns with the general understanding of deep neural networks which presumes that deeper layers can learn more complex patterns \cite{lecun2015deep}. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
In \autoref{chapter:background}, I present information that is relevant to this thesis, like the work that already has been done on this type of neural network. After providing background information, it is important to explain the most important characteristics of the data and the type of models that are used throughout this thesis which is presented in \autoref{chapter:methods-and-datasets}. Based on the data and the model type, experiments are executed which are presented in \autoref{chapter:experiments}. This thesis will end with a brief overview of possible future work and a concise conclusion.<br/>
<br/>
<span class="comment">% CHAPTER BACKGROUND</span><br/>
\chapter{Background}<span class="keyword1">\label</span>{chapter:background}<br/>
<br/>
In this chapter I will provide background information and previous established work that is relevant to this thesis. In \autoref{background:speaker-identification} I will provide an overview of what already has been done on the automatic identification of speakers. In \autoref{background:visually-grounded-speech} previous work on Visually Grounded Speech is provided while \autoref{background:gender-bias-in-machine-learning} gives an overview of literature that considered gender bias in Machine Learning.<br/>
<br/>
<span class="keyword1">\section</span>{Speaker identification}<span class="keyword1">\label</span>{background:speaker-identification}<br/>
The first attempts of speech recognition date back to at least the seventies of the previous century \cite{huang2014historical}. The urge for speaker recognition systems came from different disciplines like law enforcement (e.g. monitoring prison calls), access control (e.g. give access to a system based on voice verification) and technology firms (e.g. personalize web content)  \cite{reynolds2002overview}. The performance of first generation speech recognition systems were hindered by constraints that occurred during that era like limited computational power, lack of understanding how to cancel out background noise and lack of advanced models like neural networks or support vector machines \cite{huang2014historical}. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
Systems specifically concerned with the task of identifying speakers from a known set of speakers date back at least to the <br/>
<br/>
<span class="keyword1">\section</span>{Visually Grounded Speech}<span class="keyword1">\label</span>{background:visually-grounded-speech}<br/>
Models with speech and text transcriptions have been a popular method in Machine learning to learn from speech signals \cite{HarwathG15}. <span class="highlight-sh" title="Do not use 'In [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">In \cite</span>{HarwathG15} presents a model which substitutes the text transcriptions with relevant images. This methodology requires less supervision than text transcriptions because it is not possible to analyze the speech orthographically (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>using a writing system). This makes it harder to analyze and extract linguistic constructs because the same word pronounced by different speakers can adhere differences in the analysis. Therefore they decided to only train the model on words instead of whole sentences in order to reduce the complexity. The conclusion of this research is that a model focused on learning words with speech and accompanying images can learn syntax and semantics on word-level. An interesting factor is that this way of learning resembles the way humans learn more closely because humans are enabled to learn by just grounding stimuli in their sensory environment. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
<span class="highlight-sh" title="Do not use 'In [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">In \cite</span>{harwath2016unsupervised} they also developed a model based on Visually Grounded Speech signals and extended this idea. Contrary to \cite{HarwathG15}, this research focuses on continuous speech signals instead of just words. Like \cite{HarwathG15}, \cite{harwath2016unsupervised} grounds images and speech signals by mapping the modalities in the same semantic space, where a pair that describes the same scene has a higher similarity score than a pair which does not describe the same thing. This research revealed that the model can learn from continuous speech and can also learn semantic information. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
<span class="highlight-sh" title="Do not use 'In [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">In \cite</span>{HarwathG15} and \cite{harwath2016unsupervised} a convolutional architecture is used to train the model. Speech signals that weren't the same length were padded with zeros. <span class="highlight-sh" title="Do not use 'In [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">In \cite</span>{ChrupalaGA17} they also used images and continuous speech directly but instead of a convolutional architecture, a recurrent multi-layer architecture was used. This recurrent multi-layer architecture performs better than the previously proposed convolutional architecture. Research also revealed how the different layers encode information regarding syntax and semantics. They found that form-related aspects (e.g. how different speakers pronounce the same word) and semantics (e.g. identifying which words are important in a sentence) are better encoded in different parts of the neural network. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
<span class="highlight-sh" title="Do not use 'In [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">In \cite</span>{AlishahiBC17} they use this recurrent multi-layer architecture to analyze how phonemes are encoded. The model is trained on synthetic speech from the Microsoft Common Objects in Context (MS COCO) dataset. The results of the executed experiments, like phoneme decoding and phoneme discrimination, aligned with what has been found <span class="highlight-sh" title="Do not use 'in [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">in \cite</span>{ChrupalaGA17} that semantics are more prevalent in the deeper layers while form-related aspects are more prevalent in the first few layers. <br/>
<br/>
<span class="keyword1">\section</span>{Gender bias in Machine Learning}<span class="keyword1">\label</span>{background:gender-bias-in-machine-learning}<br/>
Multiple characteristics of the human voice have influence on how models interpret and classify speech signals. Characteristics like gender, accent and age are important to the performance of speaker identification models \cite{abdulla2001improving}. If these characteristics are insufficiently addressed it could be possible that certain speakers have lower error rates than others. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
<span class="highlight-sh" title="Do not use 'In [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">In \cite</span>{Tatman2017GenderAD} examined how accent and gender influence the results in the Automatic Speech Recognition (ASR) system of Youtube. The most important finding is that females and people from Scotland consistently obtain lower rates of accuracy than males and other accents.  Conclusions are based on a so-called ‘accent challenge’ experiment with a low amount of participants (N=80). This experiment required the participants to pronounce a list of words which can clearly discriminate between different accents within a language. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
Research on gender bias in speaker identification is limited and it is therefore not possible to give definitive answer on why differences occur (\cite{Tatman2017GenderAD}, \cite{abdulla2001improving}). One possible reason is that there is imbalance in the data. For example, it may be the case that the model of the ASR of Youtube is trained on more male than female feature vectors. Another possible reason for bias is that there are differences in pitch between males and females \cite{latinus2012discriminating}. Pitch differences occur due to biological reasons (e.g. length of the vocal tract) and the style of the speaker which may differ between male and female \cite{meena2013gender}. The difference in pitch is a factor that may contribute to performance differences, however, <span class="highlight-sh" title="Do not use 'in [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">in \cite</span>{Tatman2017GenderAD} no significant effect was found. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
Inspired by this research, an experiment in this thesis is presented which examines whether there are any performance differences between male and female in neural networks based on Visually Grounded Speech signals. There is specifically chosen for gender bias because this the only mentioned characteristic that is available for analysis. Caution must therefore be taken to interpret results because confounding variables like accent and age may influence the results. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\\</span> <br/>
<br/>
Results in this thesis cannot directly be compared to \cite{Tatman2017GenderAD} because bias is examined in a restrained environment based on words instead of continuous speech fragments. Continuous speech fragments are substantially more subject to background noise. Moreover, the experiment in this thesis differs <span class="highlight-sh" title="Do not use 'from [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">from \cite</span>{Tatman2017GenderAD} because the accuracy scores are obtained through a system which focus on speech recognition. <br/>
<br/>
<br/>
<span class="comment">% CHAPTER METHODS AND DATASETS</span><br/>
\chapter{Methods and datasets}<span class="keyword1">\label</span>{chapter:methods-and-datasets}<br/>
<br/>
This chapter specifies the specifics of the dataset and how the research is going to be validated. In \autoref{methods-and-datasets:data} there is elaborated on the key characteristics of the dataset. \autoref{methods-and-datasets:labeling-of-gender} describes the procedure that is used to label the gender of each speaker in each dataset \footnote{All the code that was used to execute the experiments and labeling the data in this thesis can be found in https://github.com/markvdlaan93/vgs-speaker-identification.}. Finally, \autoref{methods-and-datasets:models} explains which models are used and its characteristics.<br/>
<br/>
<span class="keyword1">\section</span>{Data}<span class="keyword1">\label</span>{methods-and-datasets:data}<br/>
<br/>
Contrary to the dataset used <span class="highlight-sh" title="Do not use 'in [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">in \cite</span>{AlishahiBC17}, the dataset for this thesis consists of human spoken utterances instead of synthetic speech. I started with the Flickr8K audio caption dataset \cite{HarwathG15, hodosh2013framing} which consists of 8,000 images with each five spoken captions generated by more than 180 participants in Amazon's Mechanical Turk. The Places dataset is also used in this thesis \cite{Zhou2014}. Like the Flickr8K dataset, the Places dataset also consists of human spoken utterances collected through Amazon’s Mechanical Turk but this dataset is much larger with more than 200.000 utterances and 1.400 participants and is contextually richer than the Flickr8K dataset \cite{Harwath2017LW}. This becomes clear when the audio records of the Places validation set are played: the audio records are longer, contain more words and the pauses between words are longer. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
<span class="keyword1">\section</span>{Labeling of gender}<span class="keyword1">\label</span>{methods-and-datasets:labeling-of-gender}<br/>
<br/>
To check whether a speaker is male or female, I listened for each speaker to an audio record provided in the validation dataset of Flickr8K and Places. For each speaker, to make sure no mistakes are made, the audio record is also labelled by a second listener. After the second round, the results of the first two rounds were compared and checked whether there were any differences between these two rounds. For the audio records of the speakers which didn't match in the first two rounds, another audio record is picked to determine whether the speaker is a male or female. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\\</span> <br/>
<br/>
<span class="keyword1">\section</span>{Distribution of data}<span class="keyword1">\label</span>{methods-and-datasets:distribution-of-data}<br/>
<br/>
The results of this procedure are presented in \autoref{labeling-of-gender:entries} and \autoref{labeling-of-gender:quantities}. For the Places dataset it was necessary to remove one speaker with a single entry because for some reason the audio record is not available in the package of the dataset \cite{harwath2016unsupervised} although the supplemented instructions say otherwise.<br/>
<br/>
<span class="keyword2">\begin{table}</span>[H]<br/>
<span class="keyword2">\begin{center}</span><br/>
<span class="keyword2">\begin{tabular}</span>{|l|rl|}<br/>
\hline      &amp; \bf Flickr8K  &amp; \bf Places \\ \hline<br/>
\bf Male    &amp; 2.697         &amp; 550 \\<br/>
\bf Female  &amp; 2.303         &amp; 449 \\ \hline<br/>
\bf Total   &amp; 5.000         &amp; 999 \\<br/>
\hline<br/>
<span class="keyword2">\end{tabular}</span><br/>
<span class="keyword2">\end{center}</span><br/>
<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{labeling-of-gender:entries} Amount of male and female speakers entries in validation sets.  }<br/>
<span class="keyword2">\end{table}</span><br/>
<br/>
<span class="keyword2">\begin{table}</span>[H]<br/>
<span class="keyword2">\begin{center}</span><br/>
<span class="keyword2">\begin{tabular}</span>{|l|rl|}<br/>
\hline      &amp; \bf Flickr8K  &amp; \bf Places \\ \hline<br/>
\bf Male    &amp; 79            &amp; 43 \\<br/>
\bf Female  &amp; 104           &amp; 41 \\ \hline<br/>
\bf Total   &amp; 183           &amp; 83 \\<br/>
\hline<br/>
<span class="keyword2">\end{tabular}</span><br/>
<span class="keyword2">\end{center}</span><br/>
<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{labeling-of-gender:quantities} Amount of male and female speakers.  }<br/>
<span class="keyword2">\end{table}</span><br/>
<br/>
<span class="keyword1">\section</span>{Models}<span class="keyword1">\label</span>{methods-and-datasets:models}<br/>
<br/>
This section provides an overview of the models that have I used in order to execute the experiments and see whether speaker identity and gender are encoded in the neural network. In \autoref{models:recurrent-highway-network} I will give a quick overview of the model that is used to train the data while \autoref{models:linear-models} presents the models that were used to classify each layer of the validation set.<br/>
<br/>
<span class="keyword1">\subsection</span>{Recurrent Highway Network}<span class="keyword1">\label</span>{models:recurrent-highway-network}<br/>
<br/>
The model which is used to train is called a so-called Recurrent Highway Network (RHN). Contrary to gated networks (e.g. Gated Recurrent Unit (GRU)), RHN’s can be used to enhance performance when recurrent depth needs to be increased \cite{ZillySKS16}. Full explanation of the model is provided <span class="highlight-sh" title="Do not use 'in [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">in \cite</span>{ChrupalaGA17}, I will only give a short overview here. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\\</span> <br/>
<br/>
For a model based on Visually Grounded Speech, it is necessary to have an image and speech encoder. Together the encoders need to make sure that spoken captions and images that are related to each other do have a closer distance measure than unrelated captions and images. The distance measure in this model is the cosine similarity which calculated by the following loss function:<br/>
<br/>
<span class="keyword2">\begin{eqnarray}</span><br/>
 \sum\limits_{u, i} \Bigg( \sum\limits_{u'} max[0,\alpha + d(u<span class="highlight-sh" title="There should be a space after a comma. [sh:d:001]">,i</span>) - d(u'<span class="highlight-sh" title="There should be a space after a comma. [sh:d:001]">,i</span>)] + <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\\</span> \nonumber<br/>
 \sum\limits_{u'} max[0,\alpha + d(u<span class="highlight-sh" title="There should be a space after a comma. [sh:d:001]">,i</span>) - d(u<span class="highlight-sh" title="There should be a space after a comma. [sh:d:001]">,i</span>')] \Bigg)<br/>
<span class="keyword2">\end{eqnarray}</span><br/>
<br/>
In this formula, u stands for utterance and i for image and u and i together is an image-caption pair which describe the same scene. U’ and i’ are the set of utterances and images that not describe the same scene. By taking the largest value of the subtracted cosine similarity values, the loss function delivers the image-caption pair which are the strongest related to each other. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
The image encoder used to process the images is a pre-trained VGG-16 model \cite{Simonyan14c} which is fed with vector representations of the image. The utterance encoder has the following structure:<br/>
<br/>
<span class="keyword2">\begin{eqnarray}</span><br/>
 enc_u(<span class="keyword1">\textbf</span>{u}) = unit(Attn(RHN_k_,_L (Conv_s_,_d_,_z (<span class="keyword1">\textbf</span>{u}))<br/>
<span class="keyword2">\end{eqnarray}</span><br/>
<br/>
In this case input u consists of Mel Frequency Ceptral Coefficient (MFCC) vectors and the RHN with k recurrent layers and L depth uses the output of a convolutional layer with size s, d dimensions and stride z. The output of the RHN is fed to the attention layer which calculates the average of the activations.<br/>
<br/>
<span class="keyword1">\subsection</span>{Linear models}<span class="keyword1">\label</span>{models:linear-models}<br/>
<br/>
For each layer in the trained neural network (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>convolutional layer, recurrent layers and embedding layer) I have the mean activation values for each feature vector in the dataset to my disposal. The Flickr8K dataset has a validation set of 5,000 feature vectors and the Places validation set 1,000. The task was to feed each layer of mean activations values (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>matrices of 5,000 by 1,024 for Flickr8K and matrices of 1,000 by 1,024 for the Places dataset) into a linear classifier and supplement it with the labels which I also have to my disposal. This implies that the experiments in this thesis differ from training of the model because I used strongly supervised learning as opposed to the weakly supervision of Visually Grounded Speech signals. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
The Stochastic Gradient Descent (SGD) classifier of the Python Scikit-Learn package is used in conjunction with Grid Search and Cross Validation. In \autoref{linear-models:hyperparameter-optimization} an overview is given of the parameters that I used to tune each layer. For the loss function I tried 'log' which gives Logistic Regression and 'Hinge' which gives a linear Support Vector Machine. Because of limited computational resources I decided to minimize Cross Validation only to three folds for each layer for each model. <br/>
<br/>
<span class="keyword2">\begin{table}</span>[H]<br/>
<span class="keyword2">\begin{center}</span><br/>
<span class="keyword2">\begin{tabular}</span>{|l|r|}<br/>
\hline              &amp; \bf Values                        \\ \hline<br/>
\bf Learning rate   &amp; 0.01, 0.001, 0.0001 and 0.00001   \\ \hline<br/>
\bf Loss function   &amp; Log and Hinge                     \\ \hline<br/>
\bf Penalty         &amp; $L_{1}$, $L_{2}$ and ElasticNet   \\ <br/>
\hline<br/>
<span class="keyword2">\end{tabular}</span><br/>
<span class="keyword2">\end{center}</span><br/>
<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{linear-models:hyperparameter-optimization} Values used in Grid Search in conjunction with K-fold Cross Validation.  }<br/>
<span class="keyword2">\end{table}</span><br/>
<br/>
<br/>
<span class="comment">% CHAPTER EXPERIMENTS</span><br/>
\<span class="highlight-sh" title="This chapter is very short (about 62 words). You should consider merging it with another section or make it longer. [sh:seclen]">chapter</span>{Experiments}<span class="keyword1">\label</span>{chapter:experiments}<br/>
<br/>
In this chapter an overview is provided of the experiments that are executed to see if and to what extent speaker identity is encoded in the proposed neural network. In \autoref{experiments:experimental-setup} I explain every aspect that is relevant to executing the experiments. After that I will present the results of each experiment in \autoref{experiments:results} and discuss these results in \autoref{experiments:discussion}.<br/>
<br/>
<span class="keyword1">\section</span>{Experimental setup}<span class="keyword1">\label</span>{experiments:experimental-setup}<br/>
<br/>
In \autoref{models:linear-models} I explained that each layer is trained with 3-fold Cross Validation in conjunction with Grid Search. To verify whether the model generalizes well, I decided to first split the data into a train and test set. The training set was internally split by Scikit-learn into stratified partitions of equal length. This means that this method accounts for classes imbalances presented in \autoref{methods-and-datasets:distribution-of-data}. After training the data with Cross Validation and tuning the parameters the model is trained again but this time on the entire dataset. This methodology is used for each layer for each model and the code for running the experiments is partially taken from the Scikit-learn documentation \cite{scikitlearn2017}. <br/>
<br/>
<span class="keyword1">\subsection</span>{Splitting of the datasets}<span class="keyword1">\label</span>{experimental-setup:splitting-of-the-datasets}<br/>
<br/>
To account for the small size of the Places dataset (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>each layer consists of more dimensions than feature vectors), I decided to split the train and test set in a 60\<span class="comment">%/40\% proportion. Because the amount of feature vectors is substantially larger for the Flickr8K dataset, I decided to split the experiments on the Flickr8K into 67\% for the training set on the remaining part for the test set (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>33\%). </span><br/>
<br/>
<span class="keyword1">\subsection</span>{Stratification}<span class="keyword1">\label</span>{experimental-setup:stratification}<br/>
<br/>
By default the train and test split function of Scikit-learn is not stratified \cite{scikitlearntraintestsplit}. Due to time constraints I decided to not stratify the classification of speaker identity on both the Flickr8K and Places dataset. The train and test split function can only be used in stratification mode when each class in dataset has at least two members. For gender classification this is not a problem because each class (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>male and female) has at least two members so no extra pre-processing is required. For speaker identification, this would require me to do more preprocessing which I did not have time for anymore. However, I checked whether leaving in single entries were considered in the calculation of the evaluation metrics like the accuracy and F1-score. Internally Scikit-learn ignores these observations by giving them a zero as score and not consider the single entry speakers in averaging the evaluation metric over the classes. <br/>
<br/>
<span class="keyword1">\subsection</span>{Gender bias}<span class="keyword1">\label</span>{experimental-setup:gender-bias}<br/>
<br/>
To examine if the presented neural network suffers from performance differences between males and females, it was not necessary to create a separate model for each layer in each dataset. The accuracy and F1-scores per gender are derived from the classification on gender.<br/>
<br/>
\<span class="highlight-sh" title="This subsection is very short (about 103 words). You should consider merging it with another section or make it longer. [sh:seclen]">subsection</span>{Evaluation metrics}<span class="keyword1">\label</span>{experimental-setup:evaluation-metrics}<br/>
<br/>
The Grid Search Cross Validation function of Scikit-learn only has the possibility to provide a single scoring function. I have decided to use the parameter ‘f1\_weighted' which calculates the F1-score per class and average the results based on the amount of instances in a certain class \cite{scikitlearn20172}. In \autoref{methods-and-datasets:distribution-of-data} I explained that I will provide both accuracy and F1-scores of each experiment. However, since the scores on the training folds will be F1-scores, I take this metric as guideline for interpreting the results. If there is any notable difference between the F1-score and accuracy for a layer, I will mention that.<br/>
<br/>
<span class="keyword1">\section</span>{Results}<span class="keyword1">\label</span>{experiments:results}<br/>
<br/>
For each proposed experiment I will provide the results in this section. \autoref{results:speaker-identification-in-flickr8k-dataset} starts with explaining observations for speaker identification on the Flickr8K data while \autoref{results:speaker-identification-in-places-dataset} focuses on speaker identification in the Places dataset. \autoref{results:gender-identification-in-flickr8k-dataset} and \autoref{results:gender-identification-in-places-dataset} elaborates on gender classification and \autoref{results:gender-bias} will provide results for the research regarding gender bias.<br/>
<br/>
<span class="keyword1">\subsection</span>{Speaker identification in Flickr8K dataset}<span class="keyword1">\label</span>{results:speaker-identification-in-flickr8k-dataset}<br/>
<br/>
In \autoref{table:speaker-identification-on-flickr8k-dataset} an overview is given of the optimal parameters for each layer. For each layer the optimal loss function, penalty and learning rate are presented. The column ‘Average training F1-score’ specifies the average F1-score over each fold for the optimal parameters. The standard deviation is an important indicator which provides insights in how consistent the predictions are over the different folds. For speaker identification in the Flickr8K dataset \autoref{table:speaker-identification-on-flickr8k-dataset} shows that the standard deviation is small (not greater than 0.04). This overview also shows that for each layer $L_{2}$ normalization gave the best results for regularization. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
In \autoref{table:gender-speaker-classification} the performance of each experiment is presented. For speaker identification in Flickr8K dataset the classification on the raw MFCC vectors and the convolutional layer consists of approximately the same results which is followed by a sharp increase in the first recurrent layer. After the first recurrent layer the performance drops steadily till the point that the fourth recurrent layer and embedding layer are performing even worse than the raw MFCC vectors and the convolutional layer. <br/>
<br/>
<span class="keyword1">\subsection</span>{Gender identification in the Flickr8K dataset}<span class="keyword1">\label</span>{results:gender-identification-in-flickr8k-dataset}<br/>
<br/>
Similar to the results presented in \autoref{results:speaker-identification-in-flickr8k-dataset}, the folds in the training dataset don’t have much variance and the pattern consists of relatively low scores for the MFCC and convolutional layer followed by a sharp increase (see \autoref{table:gender-speaker-classification}). Contrary to the results in \autoref{results:speaker-identification-in-flickr8k-dataset}, the second recurrent layer scores slightly better than the first layer. After the second layer the performance decreases but the drop in the embedding layer isn’t of the same magnitude as that of speaker identification in Flickr8K. This pattern is however not visible in the average F1-scores of the training dataset (see \autoref{table:gender-identification-on-flickr8k-dataset}).<br/>
<br/>
<span class="keyword1">\subsection</span>{Speaker identification identification in Places dataset}<span class="keyword1">\label</span>{results:speaker-identification-in-places-dataset}<br/>
<br/>
In \autoref{table:speaker-identification-on-places-dataset} an overview is given for the optimal parameters for each layer. Contrary to the other results the variances for speaker identification on the Places dataset is much higher (around +/- 0.2). Similar to the other experiments the pattern starts with relatively low scores for MFCC and convolutional layer followed by a sharp increase. After the sharp increase, a performance  drop is observed for the second recurrent layer followed by a small increase in third layer. The fourth recurrent and embedding layer encode speaker identification worse than the other layers.<br/>
<br/>
<span class="keyword1">\subsection</span>{Gender identification in the Places dataset}<span class="keyword1">\label</span>{results:gender-identification-in-places-dataset}<br/>
<br/>
Consistent with encoding of males and females in the flickr8K dataset, the results in \autoref{table:gender-identification-on-places-dataset} show that the variance in the training folds is low. In \autoref{discussion:speaker-identification-in-places-dataset} shows that the largest performance score is reached in the first recurrent layer followed by a relatively sharp drop which recovers in third recurrent layer. After the third recurrent layer, similar to the other experiments, the performance drops.<br/>
<br/>
<span class="keyword1">\subsection</span>{Gender bias}<span class="keyword1">\label</span>{results:gender-bias}<br/>
<br/>
The results for both the Flickr8K and Places dataset are presented in \autoref{table:gender-bias}. For both datasets and both genders the performance on the raw MFCC vectors and convolutional layer is approximately equal but low compared to the other layers. After the convolutional and MFCC layers, for each dataset and for each gender there is a large increase. Differences between gender are most prevalent in the MFCC and convolutional layer with a maximum over 0.6 favoring males in the Flickr8K dataset (F1-score of 0.7767 for males and 0.7137 for females) and 0.3 favoring females in the Places dataset (F1-score of 0.8571 and 0.8889). After the MFCC and convolutional layer, both genders in both datasets are much closer to each other.<br/>
<br/>
<span class="highlight-sh" title="If you are writing a research paper, do not force page breaks. [sh:nonp]">\</span>newpage<br/>
<span class="keyword2">\begin{table}</span>[H]<br/>
<span class="keyword2">\begin{center}</span><br/>
<span class="keyword2">\begin{tabular}</span>{|l|r|l|l|l|l|}<br/>
\hline              &amp; \bf Loss function     &amp; \bf Penalty   &amp; \bf Learning rate &amp; \bf Average training F1-score &amp; \bf Std. \\ \hline<br/>
\bf MFCC            &amp; Log                   &amp; L2            &amp; 0.001             &amp; 0.800                         &amp; 0.022 \\<br/>
\bf Convolutional   &amp; Log                   &amp; L2            &amp; 0.001             &amp; 0.803                         &amp; 0.030 \\ <br/>
\bf Recurrent 1     &amp; Hinge                 &amp; L2            &amp; 0.001             &amp; 0.925                         &amp; 0.026 \\<br/>
\bf Recurrent 2     &amp; Hinge                 &amp; L2            &amp; 0.01              &amp; 0.871                         &amp; 0.038 \\<br/>
\bf Recurrent 3     &amp; Hinge                 &amp; L2            &amp; 0.01              &amp; 0.816                         &amp; 0.039 \\<br/>
\bf Recurrent 4     &amp; Hinge                 &amp; L2            &amp; 0.01              &amp; 0.773                         &amp; 0.049 \\<br/>
\bf Embedding       &amp; Hinge                 &amp; L2            &amp; 0.0001            &amp; 0.580                         &amp; 0.031 \\<br/>
\hline<br/>
<span class="keyword2">\end{tabular}</span><br/>
<span class="keyword2">\end{center}</span><br/>
<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{table:speaker-identification-on-flickr8k-dataset} The optimal parameters for each layer for speaker identification on the Flickr8K dataset.  }<br/>
<span class="keyword2">\end{table}</span><br/>
<br/>
<span class="keyword2">\begin{table}</span>[H]<br/>
<span class="keyword2">\begin{center}</span><br/>
<span class="keyword2">\begin{tabular}</span>{|l|r|l|l|l|l|}<br/>
\hline              &amp; \bf Loss function  &amp; \bf Penalty  &amp; \bf Learning rate &amp; \bf Average training F1-score &amp; \bf Std. \\ \hline<br/>
\bf MFCC            &amp; Hinge              &amp; L2           &amp; 0.01              &amp;  0.757                        &amp; 0.027 \\<br/>
\bf Convolutional   &amp; Hinge              &amp; L1           &amp; 0.01              &amp;  0.765                        &amp; 0.024 \\ <br/>
\bf Recurrent 1     &amp; Log                &amp; Elasticnet   &amp; 0.001             &amp;  0.949                        &amp; 0.010 \\<br/>
\bf Recurrent 2     &amp; Hinge              &amp; L2           &amp; 0.01              &amp;  0.938                        &amp; 0.015 \\<br/>
\bf Recurrent 3     &amp; Hinge              &amp; L2           &amp; 0.01              &amp;  0.929                        &amp; 0.003 \\<br/>
\bf Recurrent 4     &amp; Log                &amp; L2           &amp; 0.01              &amp;  0.919                        &amp; 0.014 \\<br/>
\bf Embedding       &amp; Log                &amp; Elasticnet   &amp; 0.0001            &amp;  0.896                        &amp; 0.006 \\<br/>
\hline<br/>
<span class="keyword2">\end{tabular}</span><br/>
<span class="keyword2">\end{center}</span><br/>
<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{table:gender-identification-on-flickr8k-dataset} The optimal parameters for each layer for gender identification on the Flickr8K dataset.  }<br/>
<span class="keyword2">\end{table}</span><br/>
<br/>
<span class="keyword2">\begin{table}</span>[H]<br/>
<span class="keyword2">\begin{center}</span><br/>
<span class="keyword2">\begin{tabular}</span>{|l|r|l|l|l|l|}<br/>
\hline              &amp; \bf Loss function     &amp; \bf Penalty &amp; \bf Learning rate   &amp; \bf Average training F1-score &amp; \bf Std. \\ \hline<br/>
\bf MFCC            &amp; Log                   &amp;  Elasticnet &amp; 0.001               &amp; 0.779                         &amp; 0.220 \\<br/>
\bf Convolutional   &amp; Log                   &amp;  Elasticnet &amp; 0.001               &amp; 0.812                         &amp; 0.189 \\ <br/>
\bf Recurrent 1     &amp; Hinge                 &amp;  L2         &amp; 0.01                &amp; 0.866                         &amp; 0.197 \\<br/>
\bf Recurrent 2     &amp; Hinge                 &amp;  Elasticnet &amp; 0.01                &amp; 0.778                         &amp; 0.276 \\<br/>
\bf Recurrent 3     &amp; Hinge                 &amp;  L2         &amp; 0.01                &amp; 0.774                         &amp; 0.273 \\<br/>
\bf Recurrent 4     &amp; Hinge                 &amp;  Elasticnet &amp; 0.01                &amp; 0.775                         &amp; 0.280 \\<br/>
\bf Embedding       &amp; Hinge                 &amp;  l2         &amp; 0.0001              &amp; 0.695                         &amp; 0.264 \\<br/>
\hline<br/>
<span class="keyword2">\end{tabular}</span><br/>
<span class="keyword2">\end{center}</span><br/>
<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{table:speaker-identification-on-places-dataset} The optimal parameters for each layer for speaker identification on the Places dataset.  }<br/>
<span class="keyword2">\end{table}</span><br/>
<br/>
<span class="keyword2">\begin{table}</span>[H]<br/>
<span class="keyword2">\begin{center}</span><br/>
<span class="keyword2">\begin{tabular}</span>{|l|r|l|l|l|l|}<br/>
\hline              &amp; \bf Loss function     &amp; \bf Penalty   &amp; \bf Learning rate &amp; \bf Average training F1-score     &amp; \bf Std. \\ \hline<br/>
\bf MFCC            &amp;  Log                  &amp; L1            &amp; 0.01              &amp; 0.897                             &amp; 0.009 \\<br/>
\bf Convolutional   &amp;  Hinge                &amp; Elasticnet    &amp; 0.001             &amp; 0.897                             &amp; 0.014 \\ <br/>
\bf Recurrent 1     &amp;  Log                  &amp; L2            &amp; 0.001             &amp; 0.974                             &amp; 0.000 \\<br/>
\bf Recurrent 2     &amp;  Log                  &amp; L2            &amp; 0.001             &amp; 0.965                             &amp; 0.018 \\<br/>
\bf Recurrent 3     &amp;  Log                  &amp; Elasticnet    &amp; 0.001             &amp; 0.965                             &amp; 0.018 \\<br/>
\bf Recurrent 4     &amp;  Log                  &amp; Elasticnet    &amp; 0.001             &amp; 0.966                             &amp; 0.006 \\<br/>
\bf Embedding       &amp;  Hinge                &amp; L1            &amp; 0.001             &amp; 0.951                             &amp; 0.951 \\<br/>
\hline<br/>
<span class="keyword2">\end{tabular}</span><br/>
<span class="keyword2">\end{center}</span><br/>
<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{table:gender-identification-on-places-dataset} The optimal parameters for each layer for gender identification on the Places dataset.  }<br/>
<span class="keyword2">\end{table}</span><br/>
<br/>
<span class="highlight-sh" title="If you are writing a research paper, do not force page breaks. [sh:nonp]">\</span>newpage<br/>
<br/>
<span class="comment">% CHAPTER DISCUSSION</span><br/>
\chapter{Discussion}<span class="keyword1">\label</span>{experiments:discussion}<br/>
<br/>
In this chapter I will discuss the presented findings from \autoref{chapter:experiments}. In \autoref{discussion:speaker-identification-in-flickr8k-dataset} I will discuss how the results of speaker identification on the Flickr8K dataset should be interpreted. After that, in \autoref{discussion:gender-identification-in-flickr8k-dataset} I present results on gender identification in the Flickr8K dataset followed by the analysis of both speaker and gender identification in the Places dataset in \autoref{discussion:speaker-identification-in-places-dataset} and \autoref{dicussion:gender-identification-in-places-dataset}. This section ends with an analysis on how performance differences between male and female should be interpreted in \autoref{discussion:gender-bias}.<br/>
<br/>
<span class="keyword1">\section</span>{Speaker identification in Flickr8K dataset}<span class="keyword1">\label</span>{discussion:speaker-identification-in-flickr8k-dataset}<br/>
<br/>
The results in \autoref{results:speaker-identification-in-flickr8k-dataset} revealed that the standard deviation between the average F1-scores of the different folds is low and thus the variance is low. Based on the experiments of phoneme encoding \cite{AlishahiBC17}, I suspected that speaker identification would follow the same trend. From all the experiments that are executed in this thesis, speaker identification in Flickr8K dataset follows this pattern the most consistently. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\\</span> <br/>
<br/>
The first recurrent layer classifies the most speakers correctly followed by a descending curve. In \autoref{experimental-setup:stratification} I mentioned that, due to time constraints, I didn’t use stratification option in the procedure of splitting between the test and train set. This may causes poor generalizability because the speakers aren’t equally divided over the train and test set. If so, then the average F1-score over the folds and the F1-score over the test set should differ significantly which is not the case. Another finding is that the classification on the convolutional layer and MFCC vector follow the same pattern as the experiments presented in phoneme encoding \cite{AlishahiBC17}.<br/>
<br/>
<span class="keyword1">\section</span>{Gender identification in Flickr8K dataset}<span class="keyword1">\label</span>{discussion:gender-identification-in-flickr8k-dataset}<br/>
<br/>
The prevalence of gender encoding in the Flickr8K dataset is, contrary to the other experiments, most notable in the second recurrent layer of the model. Although the second recurrent layer scores better than the first layer, the difference between the two layers is negligible (only 0.0012 difference). For gender identification in the Flickr8K dataset I therefore conclude that performance for the first two recurrent layers are the same. Consistent with results in the other experiments, the performance drops after the first two recurrent layers. Comparing the performance in the training (see \autoref{table:gender-identification-on-flickr8k-dataset}) and test set (see \autoref{table:gender-speaker-classification} and \autoref{appendix:difference-between-training-and-test-scores}) reveals that confirmation of the hypothesis is more evident in the training dataset. Why this difference occurs is not certain because for the experiment on encoding of gender the train and test are stratified. Moreover, the trained model generalizes adequately because for every layer the score on the test set doesn't differ much compared to the training set (see \autoref{appendix:difference-between-training-and-test-scores}).<br/>
<br/>
<span class="keyword1">\section</span>{Speaker identification in Places dataset}<span class="keyword1">\label</span>{discussion:speaker-identification-in-places-dataset}<br/>
<br/>
Contrary to the results presented in \autoref{discussion:speaker-identification-in-flickr8k-dataset}, the presented results in  \autoref{results:gender-identification-in-places-dataset} suggest that classification on the Places dataset does suffer from more variance. This is probably caused by the limited amount of data available and the lack of stratification. With limited amount of data I mean that each fold doesn’t have a wide range of examples available for a larger portion of the speakers. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
Similar to the results showed in \autoref{discussion:speaker-identification-in-flickr8k-dataset}, the first recurrent layer gives the best results. However, the trend after the first recurrent layer is less similar than speaker identification on the Flickr8K dataset but there is still a downwards trend. Especially if only the average F1-score of the folds is considered in \autoref{table:speaker-identification-on-places-dataset} the results confirm the hypothesis. Moreover, if the difference between training and test F1-score for each layer are compared, there can be concluded that scores don’t differ much. In fact, the trained model for each layer generalizes well and in most cases only differ with +/- 0.01. Consistent with the average F1-scores of the training data, the accuracy scores in \autoref{table:gender-speaker-classification-accuracy} also show an exclusively descending line starting from first recurrent layer.<br/>
<br/>
<span class="keyword1">\section</span>{Gender identification in the Places dataset}<span class="keyword1">\label</span>{dicussion:gender-identification-in-places-dataset}<br/>
<br/>
Compared to the experiment on encoding of gender on the Flickr8K dataset, encoding of gender is in every layer in the Places dataset more notable. Why gender is better encoded in the Places dataset is not certain. You could argue that the Places dataset is contextually richer which leads to better performance (<span class="highlight-sh" title="Use a backslash after the second period; otherwise LaTeX will think it is a full stop ending a sentence. [sh:011]">i.e. </span>the model has more information to its disposal). However this is not entirely consistent with the experiments on speaker identification in both datasets. Except for the embedding layer, encoding of speakers is in all layers more notable in the Flickr8K dataset than in the Places dataset. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
Similar to the other experiments generalizability the experiment on encoding of males and females in the Places dataset adequate. For some layers the performance is slightly lower (e.g. the fourth recurrent layer has a F1-score of 0.966 compared to 0.9575 in the test set) than in the training set. <br/>
<br/>
<span class="keyword1">\section</span>{Gender bias}<span class="keyword1">\label</span>{discussion:gender-bias}<br/>
<br/>
Inspired by the previously executed experiments \cite{Tatman2017GenderAD}, I decided to research whether gender bias occurs in neural networks based on Visually Grounded Speech signals. The results for the convolutional and MFCC layer of both datasets have conflicting scores. In the Flickr8K dataset the first layers score consistently better for male speakers than for female speakers while for the places dataset it is the other way around. Analyzing all layers, this pattern continue to occur although the differences in the recurrent and embedding layers are negligible. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\\</span> <br/>
<br/>
It is hard to say why theses differences occur. Although research cannot be directly compared, <span class="highlight-sh" title="Do not use 'in [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">in \cite</span>{Tatman2017GenderAD} it became clear that unbalanced classes may be an issue which could cause differences in performance. The distribution of Places and Flickr8K dataset are already given in \autoref{methods-and-datasets:distribution-of-data}. Analyzing the differences in gender between the datasets reveal that Flickr8K consists of more males and Places consists of more females. This can possibly clarify why gender scores for the different datasets give different results: there is bias in the data. Although indicates \autoref{labeling-of-gender:entries} that the amount of feature vectors does not differ much between male and female, the algorithm gets a more diverse set of male speakers in the Flickr8K dataset and female speakers in the Places dataset (see \autoref{labeling-of-gender:quantities}). Even though the data is stratified, it does not account for this issue because stratification only makes sure that the split groups consist of an equal amount of male and female examples. Techniques like oversampling and undersampling can be potentially used to solve this problem \cite{wiki:Oversamplingundersampling}.<br/>
<br/>
<span class="keyword2">\begin{figure}</span>[h!]<br/>
	\centering<br/>
	<span class="keyword1">\includegraphics</span>[scale=0.8]{images/gender-speaker-classification.png}\\<br/>
	<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure table:gender-speaker-classification is never referenced in the text [sh:figref]">t</span>able:gender-speaker-classification} F1-scores for each experiment.  }<br/>
<span class="keyword2">\end{figure}</span><br/>
<br/>
<span class="keyword2">\begin{figure}</span>[h!]<br/>
	\centering<br/>
	<span class="keyword1">\includegraphics</span>[scale=0.8]{images/gender-bias.png}\\<br/>
	<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{table:gender-bias} F1-scores for gender in both the Places and Flickr8K dataset.  }<br/>
<span class="keyword2">\end{figure}</span><br/>
<br/>
<span class="highlight-sh" title="If you are writing a research paper, do not force page breaks. [sh:nonp]">\newpage</span>	<br/>
	<br/>
<br/>
<span class="comment">% CHAPTER FUTURE WORK AND CONCLUSION</span><br/>
\chapter{Future work and conclusion}<span class="keyword1">\label</span>{chapter:future-work-and-conclusion}<br/>
<br/>
In this chapter I will present future work in \autoref{future-work-and-conclusion:future-work} that is necessary to further examine the extent to which speaker and gender encoding are processed by RNN's based on Visually Grounded Speech. I will also give a final conclusion based on the results presented in \autoref{chapter:experiments} and \autoref{experiments:discussion}.<br/>
<br/>
<span class="keyword1">\section</span>{Future work}<span class="keyword1">\label</span>{future-work-and-conclusion:future-work}<br/>
<br/>
In thesis I used strongly supervised learning to classify speakers by training the linear model with labels of each speaker. By using a Gradient Reversal layer \cite{ganin2016domain} the representation of the speaker can be weakened so that the model is trained and evaluated with less supervision. It would be interesting to see how this affect the performance scores of each layer in each dataset. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
I mentioned in \autoref{experimental-setup:stratification} that I didn't had any time left to stratify the experiments for speaker identification. Unstratified train and test set could potentially explain why the pattern is less clear in the test set. In future work it is therefore important to research how this limitation affect the performance.<br/>
<br/>
<span class="keyword1">\section</span>{Conclusion}<span class="keyword1">\label</span>{future-work-and-conclusion:conclusion}<br/>
<br/>
In this thesis I presented novel research on the prevalence of gender and speaker encoding in Recurrent Neural Networks based on Visually Grounded Speech signals. To examine how gender encoding involves in this type of Neural Network, it was necessary label all speakers manually into male and female classes. The averaged values of the hidden activations for each layer of the trained Recurrent Neural Network constructed <span class="highlight-sh" title="Do not use 'in [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">in \cite</span>{ChrupalaGA17} were used. For each layer these values were fed to a linear classifier to determine to what extent encoding of speaker and gender are notable. Various parameters were tried with help of Grid Search to find the optimal score for each layer. After classification on the Flickr8K dataset, the same procedure was applied to the Places dataset to see how the neural network handles a dataset which has longer audio records. Inspired by other research I also decided to examine whether the neural network suffers from any consistent performance differences between males and females. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
Based on the results in \autoref{chapter:experiments} and \autoref{experiments:discussion}, I conclude that in general the hypothesis is confirmed. Especially taken the F1-scores of the training set into consideration, speaker and gender encoding are most prevalent in the first few layers of the recurrent neural network. The differences are however less notable when you look to the specifics of each experiment. For example, gender encoding in the Flickr8K dataset is slightly more notable in the second recurrent layer than in the first recurrent layer. Moreover, performance on test set for speaker encoding in the Places dataset has a sharp drop in the second recurrent layer although this is not consistent with the results from the training set and accuracy scores. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
For encoding of males and females the best results were achieved in the Places dataset in all layers. Contrary to gender encoding, speaker encoding follows a different pattern namely that encoding of speakers achieve the best results in the Flickr8K dataset. Comparing encoding of gender and speaker identity the most notable finding is that the prevalence of speaker encoding descends more quickly through the recurrent layers than gender encoding. <span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\<br/>
<br/>
Research on gender bias revealed that although there are differences for gender classification in both datasets, this is most likely caused by the representation of the data.<br/>
<br/>
\bibliographystyle{apacite}<br/>
\bibliography{sources}<br/>
<br/>
<span class="keyword2">\begin{appendices}</span><br/>
\chapter{Accuracy scores for experiments}<span class="keyword1">\label</span>{accuracy-scores-for-experiments}<br/>
<br/>
<span class="keyword2">\begin{figure}</span>[h!]<br/>
	\centering<br/>
	<span class="keyword1">\includegraphics</span>[scale=0.5]{images/gender-speaker-classification-accuracy.png}\\<br/>
	<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure table:gender-speaker-classification-accuracy is never referenced in the text [sh:figref]">t</span>able:gender-speaker-classification-accuracy} Accuracy scores for each experiment.  }<br/>
<span class="keyword2">\end{figure}</span><br/>
<br/>
<span class="keyword2">\begin{figure}</span>[h!]<br/>
	\centering<br/>
	<span class="keyword1">\includegraphics</span>[scale=0.5]{images/gender-bias-accuracy.png}\\<br/>
	<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{<span class="highlight-sh" title="Figure table:gender-bias is never referenced in the text [sh:figref]">t</span>able:gender-bias} Accuracy scores for gender in both the Places and Flickr8K dataset.  }<br/>
<span class="keyword2">\end{figure}</span><br/>
<br/>
\chapter{Difference between training and test scores}<span class="keyword1">\label</span>{appendix:difference-between-training-and-test-scores}<br/>
<br/>
<span class="keyword2">\begin{table}</span>[H]<br/>
<span class="keyword2">\begin{center}</span><br/>
<span class="keyword2">\begin{tabular}</span>{|l|r|l|l|l|l|}<br/>
\hline              &amp; \bf Training F1-score &amp; \bf Test F1-score &amp; \bf Difference  \\ \hline<br/>
\bf MFCC            &amp; 0.800                 &amp; 0.8049            &amp; 0.0113 \\<br/>
\bf Convolutional   &amp; 0.803                 &amp; 0.8143            &amp; 0.0113 \\ <br/>
\bf Recurrent 1     &amp; 0.925                 &amp; 0.9313            &amp; 0.0063 \\<br/>
\bf Recurrent 2     &amp; 0.871                 &amp; 0.8756            &amp; 0.0046 \\<br/>
\bf Recurrent 3     &amp; 0.816                 &amp; 0.8396            &amp; 0.0236 \\<br/>
\bf Recurrent 4     &amp; 0.773                 &amp; 0.8042            &amp; 0.0312 \\<br/>
\bf Embedding       &amp; 0.580                 &amp; 0.6055            &amp; 0.0255 \\<br/>
\hline<br/>
<span class="keyword2">\end{tabular}</span><br/>
<span class="keyword2">\end{center}</span><br/>
<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{table:exact-difference-flickr8k-speaker} Exact difference (test - training = difference) in F1-score between prediction on the training and test set for the Flickr8K speaker identification experiment.  }<br/>
<span class="keyword2">\end{table}</span><br/>
<br/>
<span class="keyword2">\begin{table}</span>[H]<br/>
<span class="keyword2">\begin{center}</span><br/>
<span class="keyword2">\begin{tabular}</span>{|l|r|l|l|l|l|}<br/>
\hline              &amp; \bf Training F1-score &amp; \bf Test F1-score &amp; \bf Difference  \\ \hline<br/>
\bf MFCC            &amp; 0.757                 &amp; 0.7477            &amp; -0.0093 \\<br/>
\bf Convolutional   &amp; 0.765                 &amp; 0.7520            &amp; -0.0130 \\ <br/>
\bf Recurrent 1     &amp; 0.949                 &amp; 0.9552            &amp; 0.0062 \\<br/>
\bf Recurrent 2     &amp; 0.938                 &amp; 0.9564            &amp; 0.0184 \\<br/>
\bf Recurrent 3     &amp; 0.929                 &amp; 0.9418            &amp; 0.0128 \\<br/>
\bf Recurrent 4     &amp; 0.919                 &amp; 0.9339            &amp; 0.0149 \\<br/>
\bf Embedding       &amp; 0.896                 &amp; 0.9054            &amp; 0.0094 \\<br/>
\hline<br/>
<span class="keyword2">\end{tabular}</span><br/>
<span class="keyword2">\end{center}</span><br/>
<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{table:exact-difference-flickr8k-gender} Exact difference (test - training = difference) in F1-score between prediction on the training and test set for the Flickr8K gender identification experiment.  }<br/>
<span class="keyword2">\end{table}</span><br/>
<br/>
<span class="keyword2">\begin{table}</span>[H]<br/>
<span class="keyword2">\begin{center}</span><br/>
<span class="keyword2">\begin{tabular}</span>{|l|r|l|l|l|l|}<br/>
\hline              &amp; \bf Training F1-score &amp; \bf Test F1-score &amp; \bf Difference  \\ \hline<br/>
\bf MFCC            &amp; 0.779                 &amp; 0.7695            &amp; -0.0095 \\<br/>
\bf Convolutional   &amp; 0.812                 &amp; 0.8026            &amp; -0.0094 \\ <br/>
\bf Recurrent 1     &amp; 0.866                 &amp; 0.8544            &amp; -0.0116 \\<br/>
\bf Recurrent 2     &amp; 0.778                 &amp; 0.7836            &amp; 0.0056 \\<br/>
\bf Recurrent 3     &amp; 0.774                 &amp; 0.7979            &amp; 0.0239 \\<br/>
\bf Recurrent 4     &amp; 0.775                 &amp; 0.7814            &amp; 0.0064 \\<br/>
\bf Embedding       &amp; 0.695                 &amp; 0.7329            &amp; 0.0379 \\<br/>
\hline<br/>
<span class="keyword2">\end{tabular}</span><br/>
<span class="keyword2">\end{center}</span><br/>
<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{table:exact-difference-places-speaker} Exact difference (test - training = difference) in F1-score between prediction on the training and test set for the Places speaker identification experiment.  }<br/>
<span class="keyword2">\end{table}</span><br/>
<br/>
<span class="keyword2">\begin{table}</span>[H]<br/>
<span class="keyword2">\begin{center}</span><br/>
<span class="keyword2">\begin{tabular}</span>{|l|r|l|l|l|l|}<br/>
\hline              &amp; \bf Training F1-score &amp; \bf Test F1-score &amp; \bf Difference  \\ \hline<br/>
\bf MFCC            &amp; 0.897                 &amp; 0.8647            &amp; -0.0323 \\<br/>
\bf Convolutional   &amp; 0.897                 &amp; 0.8746            &amp; -0.0224 \\ <br/>
\bf Recurrent 1     &amp; 0.974                 &amp; 0.9750            &amp; 0.001 \\<br/>
\bf Recurrent 2     &amp; 0.965                 &amp; 0.9600            &amp; -0.0065 \\<br/>
\bf Recurrent 3     &amp; 0.965                 &amp; 0.9675            &amp; 0.0025 \\<br/>
\bf Recurrent 4     &amp; 0.966                 &amp; 0.9575            &amp; -0.0085 \\<br/>
\bf Embedding       &amp; 0.951                 &amp; 0.9249            &amp; -0.0261 \\<br/>
\hline<br/>
<span class="keyword2">\end{tabular}</span><br/>
<span class="keyword2">\end{center}</span><br/>
<span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{table:exact-difference-places-gender} Exact difference (test - training = difference) in F1-score between prediction on the training and test set for the Places gender identification experiment.  }<br/>
<span class="keyword2">\end{table}</span><br/>
<br/>
<span class="keyword2">\end{appendices}</span><br/>
<br/>
<span class="keyword2">\end{document}</span><br/>
</div>
<hr/>
Output produced by TeXtidote, &copy; 2018 Sylvain Hall&eacute; - All rights reserved.<br/>
See the <a href="https://sylvainhalle.github.io/sylvainhalle/textidote">TeXtidote website</a> for more information.
</body>
</html>
